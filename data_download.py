#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Date: 2025/3/21
Desc: ETFæ•°æ®ä¸‹è½½è„šæœ¬ - æ™ºèƒ½å¤„ç†å·²å­˜åœ¨æ•°æ®
"""

import os
import sys
import json
import time
from datetime import datetime
import logging
import argparse
from typing import List, Tuple, Any, Optional

# æ·»åŠ æ¨¡å—æœç´¢è·¯å¾„ï¼ˆæ ¹æ®å®é™…ç»“æ„è°ƒæ•´ï¼‰
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'litteStocks'))

try:
    from litteStocks.etf_data_manager import ETFDataDownloader
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}")
    print("è¯·ç¡®ä¿ etf_data_manager.py ä½äº litteStocks ç›®å½•ä¸‹")
    sys.exit(1)

def setup_global_logger(log_level=logging.INFO):
    """é…ç½®å…¨å±€æ—¥å¿—"""
    logger = logging.getLogger()
    logger.setLevel(log_level)
    
    # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
    if logger.handlers:
        return logger
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    return logger

def backup_existing_data(download_dir: str, backup_dir: Optional[str] = None) -> Optional[str]:
    """å¤‡ä»½å·²å­˜åœ¨çš„ETFæ•°æ®"""
    if not os.path.exists(download_dir):
        os.makedirs(download_dir)
        return None
    
    if backup_dir is None:
        backup_dir = os.path.join(download_dir, 'backups')
    
    if not os.path.exists(backup_dir):
        os.makedirs(backup_dir)
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½ç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_subdir = os.path.join(backup_dir, f'backup_{timestamp}')
    os.makedirs(backup_subdir)
    
    # å¤‡ä»½CSVæ–‡ä»¶
    backed_up = 0
    for filename in os.listdir(download_dir):
        if filename.endswith('.csv'):
            src_path = os.path.join(download_dir, filename)
            dst_path = os.path.join(backup_subdir, filename)
            try:
                import shutil
                shutil.copy2(src_path, dst_path)
                backed_up += 1
            except Exception as e:
                logging.warning(f"å¤‡ä»½æ–‡ä»¶å¤±è´¥ {filename}: {str(e)}")
    
    if backed_up > 0:
        logging.info(f"âœ… å·²å¤‡ä»½ {backed_up} ä¸ªETFæ–‡ä»¶åˆ°: {backup_subdir}")
        # ä¿ç•™æœ€è¿‘5ä¸ªå¤‡ä»½
        backup_dirs = sorted([
            d for d in os.listdir(backup_dir) 
            if os.path.isdir(os.path.join(backup_dir, d)) and d.startswith('backup_')
        ], reverse=True)
        
        for old_backup in backup_dirs[5:]:
            old_backup_path = os.path.join(backup_dir, old_backup)
            try:
                import shutil
                shutil.rmtree(old_backup_path)
                logging.debug(f"ğŸ§¹ å·²æ¸…ç†æ—§å¤‡ä»½: {old_backup_path}")
            except Exception as e:
                logging.warning(f"æ¸…ç†æ—§å¤‡ä»½å¤±è´¥ {old_backup_path}: {str(e)}")
    
    return backup_subdir if backed_up > 0 else None

def verify_existing_data(downloader: Any) -> List[Tuple[str, str]]:
    """éªŒè¯å·²å­˜åœ¨æ•°æ®çš„å®Œæ•´æ€§"""
    existing_etfs = downloader.get_existing_etfs()
    logging.info(f"ğŸ” æ­£åœ¨éªŒè¯ {len(existing_etfs)} ä¸ªå·²å­˜åœ¨ETFæ–‡ä»¶çš„å®Œæ•´æ€§...")
    
    # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨ï¼Œç¡®ä¿ç±»å‹ä¸€è‡´
    invalid_files: List[Tuple[str, str]] = []
    
    for i, symbol in enumerate(existing_etfs, 1):
        filename = [f for f in os.listdir(downloader.download_dir) 
                   if f.startswith(f"{symbol}_") and f.endswith('.csv')]
        if not filename:
            continue
        
        filepath = os.path.join(downloader.download_dir, filename[0])
        try:
            # å°è¯•è¯»å–æ–‡ä»¶
            last_date = downloader.get_last_date_from_file(filepath)
            if not last_date:
                invalid_files.append((symbol, "æ— æ³•è·å–æœ€åæ—¥æœŸ"))
            else:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(filepath)
                if file_size < 1000:  # å°äº1KBå¯èƒ½æœ‰é—®é¢˜
                    invalid_files.append((symbol, f"æ–‡ä»¶è¿‡å° ({file_size}å­—èŠ‚)"))
        except Exception as e:
            invalid_files.append((symbol, f"è¯»å–é”™è¯¯: {str(e)}"))
        
        if i % 10 == 0:
            logging.debug(f"  å·²éªŒè¯ {i}/{len(existing_etfs)} ä¸ªæ–‡ä»¶...")
    
    # ç¡®ä¿ invalid_files å§‹ç»ˆæ˜¯åˆ—è¡¨ç±»å‹
    if not isinstance(invalid_files, list):
        invalid_files = []
    
    if len(invalid_files) > 0:
        logging.warning(f"âš ï¸  å‘ç° {len(invalid_files)} ä¸ªå¯èƒ½æœ‰é—®é¢˜çš„ETFæ–‡ä»¶:")
        # ç¡®ä¿æˆ‘ä»¬åªè¿­ä»£åˆ—è¡¨ç±»å‹
        for i, item in enumerate(invalid_files[:10], 1):
            # é¢å¤–ä¿æŠ¤ï¼šç¡®ä¿itemæ˜¯å…ƒç»„
            if isinstance(item, tuple) and len(item) >= 2:
                symbol, reason = item[0], item[1]
            else:
                symbol, reason = str(item), "æ ¼å¼é”™è¯¯"
            logging.warning(f"  {i}. {symbol}: {reason}")
        
        if len(invalid_files) > 10:
            logging.warning(f"  ... è¿˜æœ‰ {len(invalid_files) - 10} ä¸ªæ–‡ä»¶æœ‰é—®é¢˜")

    return invalid_files

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ETFæ•°æ®ä¸‹è½½å·¥å…·')
    parser.add_argument('--full', action='store_true', help='å¼ºåˆ¶å…¨é‡ä¸‹è½½ï¼ˆå¿½ç•¥å·²æœ‰æ•°æ®ï¼‰')
    parser.add_argument('--update', action='store_true', help='ä»…æ›´æ–°å·²æœ‰ETFï¼ˆå¢é‡æ›´æ–°ï¼‰')
    parser.add_argument('--symbol', type=str, help='æŒ‡å®šå•ä¸ªETFä»£ç ä¸‹è½½/æ›´æ–°ï¼Œä¾‹å¦‚: 513500')
    parser.add_argument('--symbols-file', type=str, help='ä»æ–‡ä»¶è¯»å–ETFä»£ç åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªä»£ç ')
    parser.add_argument('--backup', action='store_true', help='åœ¨æ“ä½œå‰å¤‡ä»½ç°æœ‰æ•°æ®')
    parser.add_argument('--verify', action='store_true', help='éªŒè¯ç°æœ‰æ•°æ®å®Œæ•´æ€§')
    parser.add_argument('--log-level', type=str, default='INFO', 
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                        help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--download-dir', type=str, default='download',
                        help='ä¸‹è½½ç›®å½•è·¯å¾„')
    parser.add_argument('--progress-file', type=str, 
                        default='litteStocks/etf_download_progress.json',
                        help='è¿›åº¦æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    log_level = getattr(logging, args.log_level.upper())
    logger = setup_global_logger(log_level)
    
    logging.info("=" * 60)
    logging.info("ğŸš€ ETFæ•°æ®ä¸‹è½½å·¥å…·å¯åŠ¨")
    logging.info(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logging.info(f"ä¸‹è½½ç›®å½•: {os.path.abspath(args.download_dir)}")
    logging.info(f"è¿›åº¦æ–‡ä»¶: {os.path.abspath(args.progress_file)}")
    logging.info("=" * 60)
    
    # å¤‡ä»½ç°æœ‰æ•°æ®
    if args.backup:
        backup_dir = backup_existing_data(args.download_dir)
        if backup_dir:
            logging.info(f"ğŸ’¾ å¤‡ä»½å·²å®Œæˆï¼Œå¤‡ä»½ä½ç½®: {backup_dir}")
    
    # éªŒè¯ç°æœ‰æ•°æ®
    if args.verify:
        downloader = ETFDataDownloader(
            download_dir=args.download_dir,
            progress_file=args.progress_file,
            log_level=log_level
        )
        invalid_files = verify_existing_data(downloader)
        if invalid_files:
            logging.warning(f"å»ºè®®ä¿®å¤æˆ–åˆ é™¤ {len(invalid_files)} ä¸ªæœ‰é—®é¢˜çš„æ–‡ä»¶")
        else:
            logging.info("âœ… æ‰€æœ‰ç°æœ‰ETFæ–‡ä»¶éªŒè¯é€šè¿‡")
    
    # ç¡®å®šè¿è¡Œæ¨¡å¼
    mode = "auto"
    if args.full:
        mode = "full"
        logging.info("ğŸ”„ å°†æ‰§è¡Œå…¨é‡ä¸‹è½½ï¼ˆå¿½ç•¥å·²æœ‰æ•°æ®ï¼‰")
    elif args.update:
        mode = "update"
        logging.info("ğŸ”„ å°†æ‰§è¡Œå¢é‡æ›´æ–°ï¼ˆåªæ›´æ–°å·²æœ‰ETFï¼‰")
    else:
        logging.info("ğŸ”„ å°†è‡ªåŠ¨åˆ¤æ–­è¿è¡Œæ¨¡å¼")
    
    # ç¡®å®šETFåˆ—è¡¨
    symbols = None
    if args.symbol:
        symbols = [args.symbol.strip()]
        logging.info(f"ğŸ¯ å°†å¤„ç†æŒ‡å®šETF: {args.symbol}")
    elif args.symbols_file and os.path.exists(args.symbols_file):
        with open(args.symbols_file, 'r', encoding='utf-8') as f:
            symbols = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        logging.info(f"ğŸ“‹ ä»æ–‡ä»¶åŠ è½½ {len(symbols)} ä¸ªETFä»£ç ")
    
    try:
        # åˆå§‹åŒ–ä¸‹è½½å™¨
        downloader = ETFDataDownloader(
            download_dir=args.download_dir,
            progress_file=args.progress_file,
            max_retries=3,
            retry_delay=2.0,
            log_level=log_level
        )
        
        # åŒæ­¥å·²æœ‰æ–‡ä»¶åˆ°è¿›åº¦è®°å½•
        if not args.full:  # å…¨é‡ä¸‹è½½æ—¶ä¸éœ€è¦åŒæ­¥
            existing_files = downloader.get_existing_etfs()
            newly_added = 0
            for symbol in existing_files:
                if symbol not in downloader.downloaded_etfs:
                    downloader.downloaded_etfs.add(symbol)
                    if symbol in downloader.failed_etfs:
                        downloader.failed_etfs.remove(symbol)
                    newly_added += 1
            
            if newly_added > 0:
                downloader._save_progress()
                logging.info(f"âœ… è‡ªåŠ¨åŒæ­¥ {newly_added} ä¸ªæ–°å‘ç°çš„ETFæ–‡ä»¶åˆ°è¿›åº¦è®°å½•")
        
        # æ‰§è¡Œä¸‹è½½/æ›´æ–°
        logging.info("\n" + "=" * 60)
        logging.info(f"âš¡ å¼€å§‹æ‰§è¡Œ {mode} æ“ä½œ...")
        logging.info("=" * 60)
        
        start_time = time.time()
        result = downloader.run(mode=mode, symbols=symbols)
        elapsed_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        logging.info("\n" + "=" * 60)
        logging.info("âœ… ä»»åŠ¡å®Œæˆ!")
        logging.info(f"æˆåŠŸ: {result['success_count']}, å¤±è´¥: {result['fail_count']}")
        
        if 'total_new_records' in result:
            logging.info(f"æ–°å¢è®°å½•: {result['total_new_records']}")
        
        logging.info(f"è€—æ—¶: {elapsed_time:.1f} ç§’")
        logging.info(f"è¿›åº¦å·²ä¿å­˜åˆ°: {args.progress_file}")
        logging.info("=" * 60)
        
        # æ˜¾ç¤ºå¤±è´¥åˆ—è¡¨
        if result.get('fail_count', 0) > 0 and result.get('failed_symbols'): # type: ignore
            logging.error("\nâŒ å¤±è´¥çš„ETFä»£ç :")
            for i, symbol in enumerate(result['failed_symbols'], 1):  # type: ignore 
                logging.error(f"  {i}. {symbol}")
            
            logging.error(f"\nå¯åœ¨æ—¥å¿—æ–‡ä»¶ä¸­æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")
            logging.error(f"è¿›åº¦æ–‡ä»¶ä½ç½®: {os.path.abspath(args.progress_file)}")
        
        # ç»Ÿè®¡æœ€ç»ˆçŠ¶æ€
        final_existing = downloader.get_existing_etfs()
        logging.info(f"\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
        logging.info(f"  æ€»ETFæ–‡ä»¶æ•°: {len(final_existing)}")
        logging.info(f"  æœ¬æ¬¡æˆåŠŸå¤„ç†: {result['success_count']}")
        
        # ä¿å­˜æœ€ç»ˆè¿›åº¦
        downloader._save_progress()
        
        return 0
    
    except KeyboardInterrupt:
        logging.warning("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œè¿›åº¦å·²è‡ªåŠ¨ä¿å­˜")
        logging.warning("ä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­")
        return 1
    except Exception as e:
        logging.exception(f"\nâŒ ä¸¥é‡é”™è¯¯: {str(e)}")
        logging.error("è¿›åº¦å·²ä¿å­˜ï¼Œä¿®å¤é—®é¢˜åå¯ç»§ç»­è¿è¡Œ")
        return 1

def quick_start_example():
    """å¿«é€Ÿå…¥é—¨ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸ’¡ å¿«é€Ÿå…¥é—¨ç¤ºä¾‹:")
    print("=" * 60)
    print("1. å¢é‡æ›´æ–°æ‰€æœ‰å·²æœ‰ETFæ•°æ®:")
    print("   python data_download.py --update")
    print("")
    print("2. å…¨é‡ä¸‹è½½æ‰€æœ‰ETFæ•°æ®ï¼ˆå¿½ç•¥å·²æœ‰ï¼‰:")
    print("   python data_download.py --full")
    print("")
    print("3. ä¸‹è½½/æ›´æ–°æŒ‡å®šETF:")
    print("   python data_download.py --symbol 159001")
    print("")
    print("4. ä»æ–‡ä»¶æ‰¹é‡ä¸‹è½½ETF:")
    print("   # åˆ›å»ºsymbols.txtï¼Œæ¯è¡Œä¸€ä¸ªä»£ç ï¼Œæ”¯æŒæ³¨é‡Š")
    print("   echo 159001 > symbols.txt")
    print("   echo 510300 >> symbols.txt")
    print("   echo #512880 >> symbols.txt  # æ³¨é‡Šè¡Œä¼šè¢«å¿½ç•¥")
    print("   python data_download.py --symbols-file symbols.txt")
    print("")
    print("5. å¸¦å¤‡ä»½çš„å®‰å…¨æ›´æ–°:")
    print("   python data_download.py --update --backup --verify")
    print("=" * 60)

if __name__ == "__main__":
    exit_code = main()
    
    # æ˜¾ç¤ºå¿«é€Ÿå…¥é—¨æç¤º
    if exit_code == 0:
        quick_start_example()
    
    sys.exit(exit_code)