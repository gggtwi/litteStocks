#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Date: 2025/3/21
Desc: ETFæ•°æ®ä¸‹è½½è„šæœ¬ - åŸºäºetf_data_managerå·¥å…·
"""

import os
import sys
import time
import json
import logging
import argparse
from datetime import datetime
from typing import List, Optional, Dict, Any

# æ·»åŠ æ¨¡å—æœç´¢è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

try:
    # ä»ä¸Šä¼ çš„æ–‡ä»¶å¯¼å…¥ETFDataDownloader
    from litteStocks.etf_data_manager import ETFDataDownloader
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {str(e)}")
    print("è¯·ç¡®ä¿ etf_data_manager.py ä¸æœ¬è„šæœ¬åœ¨åŒä¸€ç›®å½•")
    sys.exit(1)

def setup_logger(log_level: int = logging.INFO) -> logging.Logger:
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger("ETFDownloader")
    logger.setLevel(log_level)
    
    # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
    if logger.handlers:
        return logger
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    log_dir = os.path.join("download", "logs")
    os.makedirs(log_dir, exist_ok=True)
    file_handler = logging.FileHandler(
        os.path.join(log_dir, f"download_{datetime.now().strftime('%Y%m%d')}.log"),
        encoding="utf-8"
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    return logger

def backup_progress_file(progress_file: str) -> None:
    """å¤‡ä»½è¿›åº¦æ–‡ä»¶"""
    if not os.path.exists(progress_file):
        return
    
    backup_file = f"{progress_file}.bak"
    try:
        import shutil
        shutil.copy2(progress_file, backup_file)
        logging.info(f"ğŸ“ å·²å¤‡ä»½è¿›åº¦æ–‡ä»¶åˆ°: {backup_file}")
    except Exception as e:
        logging.warning(f"âš ï¸  å¤‡ä»½è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")

def clear_progress_file(progress_file: str) -> None:
    """æ¸…ç©ºè¿›åº¦æ–‡ä»¶ï¼Œç”¨äºå¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰ETF"""
    try:
        # å…ˆå¤‡ä»½
        backup_progress_file(progress_file)
        
        # åˆ›å»ºç©ºçš„è¿›åº¦æ–‡ä»¶
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump({
                "downloaded": [],
                "failed": [],
                "last_update": "",
                "last_update_start": ""
            }, f, ensure_ascii=False, indent=2)
        
        logging.info("ğŸ§¹ å·²æ¸…ç©ºè¿›åº¦æ–‡ä»¶ï¼Œå°†é‡æ–°ä¸‹è½½æ‰€æœ‰ETFæ•°æ®")
    except Exception as e:
        logging.error(f"âŒ æ¸…ç©ºè¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise

def validate_etf_list(downloader: ETFDataDownloader, symbols: Optional[List[str]] = None) -> List[str]:
    """éªŒè¯ETFä»£ç åˆ—è¡¨çš„æœ‰æ•ˆæ€§"""
    try:
        # è·å–æ‰€æœ‰ETFåˆ—è¡¨
        etf_df, _ = downloader._get_etf_spot_data()
        all_valid_symbols = set(etf_df["ä»£ç "].tolist())
        
        if symbols is None:
            return sorted(all_valid_symbols)
        
        # éªŒè¯æŒ‡å®šçš„ETFä»£ç 
        valid_symbols = []
        invalid_symbols = []
        
        for symbol in symbols:
            if symbol in all_valid_symbols:
                valid_symbols.append(symbol)
            else:
                invalid_symbols.append(symbol)
        
        if invalid_symbols:
            logging.warning(f"âš ï¸  {len(invalid_symbols)} ä¸ªæ— æ•ˆçš„ETFä»£ç :")
            for symbol in invalid_symbols:
                logging.warning(f"  - {symbol}")
        
        if not valid_symbols:
            logging.error("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ETFä»£ç å¯ä¾›ä¸‹è½½")
            sys.exit(1)
        
        return valid_symbols
        
    except Exception as e:
        logging.error(f"âŒ è·å–ETFåˆ—è¡¨å¤±è´¥: {str(e)}")
        sys.exit(1)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ETFæ•°æ®ä¸‹è½½å·¥å…·')
    parser.add_argument('--full', action='store_true', help='å¼ºåˆ¶å…¨é‡ä¸‹è½½ï¼ˆå¿½ç•¥å·²æœ‰æ•°æ®ï¼‰')
    parser.add_argument('--full-history', action='store_true', help='ä¸‹è½½å®Œæ•´å†å²æ•°æ®ï¼ˆæ¸…ç©ºè¿›åº¦è®°å½•ï¼‰')
    parser.add_argument('--update', action='store_true', help='ä»…æ›´æ–°å·²æœ‰ETFæ•°æ®ï¼ˆå¢é‡æ›´æ–°ï¼‰')
    parser.add_argument('--symbol', type=str, help='æŒ‡å®šå•ä¸ªETFä»£ç ï¼Œä¾‹å¦‚: 513500')
    parser.add_argument('--symbols-file', type=str, help='ä»æ–‡ä»¶è¯»å–ETFä»£ç åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªä»£ç ')
    parser.add_argument('--log-level', type=str, default='INFO',
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                        help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--download-dir', type=str, default='download',
                        help='ä¸‹è½½ç›®å½•è·¯å¾„')
    parser.add_argument('--progress-file', type=str, default='download/etf_download_progress.json',
                        help='è¿›åº¦æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--max-retries', type=int, default=3, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    parser.add_argument('--retry-delay', type=float, default=2.0, help='åŸºç¡€é‡è¯•å»¶è¿Ÿ(ç§’)')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    log_level = getattr(logging, args.log_level.upper())
    logger = setup_logger(log_level)
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸš€ ETFæ•°æ®ä¸‹è½½å·¥å…·")
    logger.info(f"ç‰ˆæœ¬: 1.0")
    logger.info(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ä¸‹è½½ç›®å½•: {os.path.abspath(args.download_dir)}")
    logger.info(f"è¿›åº¦æ–‡ä»¶: {os.path.abspath(args.progress_file)}")
    logger.info("=" * 60)
    
    # ç¡®å®šè¿è¡Œæ¨¡å¼
    mode = "auto"
    if args.full:
        mode = "full"
        logger.info(f"ğŸ”„ è¿è¡Œæ¨¡å¼: å…¨é‡ä¸‹è½½ï¼ˆ{'åŒ…å«å®Œæ•´å†å²' if args.full_history else 'è·³è¿‡å·²ä¸‹è½½'}ï¼‰")
    elif args.update:
        mode = "update"
        logger.info("ğŸ”„ è¿è¡Œæ¨¡å¼: å¢é‡æ›´æ–°ï¼ˆåªæ›´æ–°å·²æœ‰ETFï¼‰")
    else:
        logger.info("ğŸ”„ è¿è¡Œæ¨¡å¼: è‡ªåŠ¨ï¼ˆæ ¹æ®å·²æœ‰æ•°æ®åˆ¤æ–­ï¼‰")
    
    # å¤„ç†ETFåˆ—è¡¨
    symbols = None
    if args.symbol:
        symbols = [args.symbol.strip()]
        logger.info(f"ğŸ¯ æŒ‡å®šETF: {args.symbol}")
    elif args.symbols_file and os.path.exists(args.symbols_file):
        with open(args.symbols_file, 'r', encoding='utf-8') as f:
            symbols = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        logger.info(f"ğŸ“‹ ä»æ–‡ä»¶åŠ è½½ {len(symbols)} ä¸ªETFä»£ç ")
    
    try:
        # å¦‚æœéœ€è¦ä¸‹è½½å®Œæ•´å†å²ï¼Œæ¸…ç©ºè¿›åº¦æ–‡ä»¶
        if args.full and args.full_history:
            if os.path.exists(args.progress_file):
                clear_progress_file(args.progress_file)
            else:
                logger.info("â„¹ï¸  è¿›åº¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
        
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = ETFDataDownloader(
            download_dir=args.download_dir,
            progress_file=args.progress_file,
            max_retries=args.max_retries,
            retry_delay=args.retry_delay,
            log_level=log_level
        )
        
        # éªŒè¯ETFåˆ—è¡¨
        if symbols:
            symbols = validate_etf_list(downloader, symbols)
        
        # æ‰§è¡Œä¸‹è½½/æ›´æ–°
        logger.info("\n" + "=" * 60)
        logger.info(f"âš¡ å¼€å§‹æ‰§è¡Œ {mode} æ“ä½œ")
        logger.info("=" * 60)
        
        start_time = time.time()
        result = downloader.run(mode=mode, symbols=symbols)
        elapsed_time = time.time() - start_time
        
        # æ˜¾ç¤ºç»“æœ
        logger.info("\n" + "=" * 60)
        logger.info("âœ… ä»»åŠ¡å®Œæˆ!")
        logger.info(f"æˆåŠŸ: {result['success_count']}, å¤±è´¥: {result['fail_count']}")
        
        if 'total_new_records' in result:
            logger.info(f"æ–°å¢è®°å½•: {result['total_new_records']}")
        
        logger.info(f"è€—æ—¶: {elapsed_time:.1f} ç§’")
        logger.info(f"è¿›åº¦å·²ä¿å­˜åˆ°: {args.progress_file}")
        logger.info("=" * 60)
        
        # æ˜¾ç¤ºå¤±è´¥åˆ—è¡¨
        if result.get('fail_count', 0) > 0: # type: ignore
            failed_symbols = result.get('failed_symbols', [])
            logger.error(f"\nâŒ {len(failed_symbols)} ä¸ªETFä¸‹è½½å¤±è´¥:") # type: ignore
            for i, symbol in enumerate(failed_symbols[:10], 1): # type: ignore
                logger.error(f"  {i}. {symbol}")
             
            if len(failed_symbols) > 10: # type: ignore
                logger.error(f"  ... è¿˜æœ‰ {len(failed_symbols)-10} ä¸ªå¤±è´¥çš„ETF") # type: ignore
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        existing_etfs = downloader.get_existing_etfs()
        logger.info(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f"  æ€»ETFæ•°é‡: {len(existing_etfs)}")
        logger.info(f"  æœ¬æ¬¡æˆåŠŸ: {result['success_count']}")
        
        # å¿«é€Ÿå…¥é—¨æç¤º
        if result.get('success_count', 0) > 0: # type: ignore
            logger.info("\nğŸ’¡ å¿«é€Ÿå…¥é—¨æç¤º:")
            logger.info("  - è¦æ›´æ–°å·²æœ‰æ•°æ®: python data_download.py --update")
            logger.info("  - è¦ä¸‹è½½æ‰€æœ‰ETF: python data_download.py --full")
            logger.info("  - è¦ä¸‹è½½å®Œæ•´å†å²: python data_download.py --full --full-history")
            logger.info("  - è¦ä¸‹è½½ç‰¹å®šETF: python data_download.py --symbol 513500")
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œè¿›åº¦å·²è‡ªåŠ¨ä¿å­˜")
        logger.warning("ä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­")
        return 1
    except Exception as e:
        logger.exception(f"\nâŒ ä¸¥é‡é”™è¯¯: {str(e)}")
        logger.error("è¿›åº¦å·²ä¿å­˜ï¼Œä¿®å¤é—®é¢˜åå¯ç»§ç»­è¿è¡Œ")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)