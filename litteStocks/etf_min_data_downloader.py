#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Date: 2025/3/22
Desc: ETFåˆ†é’Ÿçº§æ•°æ®ä¸‹è½½å·¥å…· - åŸºäºakshare
"""

import os
import sys
import time
import json
import logging
import argparse
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any, Tuple, Set, Union
import pandas as pd
import akshare as ak

# æ·»åŠ æ¨¡å—æœç´¢è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)


# é…ç½®æ—¥å¿—
def setup_logger(log_level: int = logging.INFO) -> logging.Logger:
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger("ETFMinuteDownloader")
    logger.setLevel(log_level)

    # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
    if logger.handlers:
        return logger

    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # æ–‡ä»¶å¤„ç†å™¨
    log_dir = os.path.join("download", "min", "logs")
    os.makedirs(log_dir, exist_ok=True)
    file_handler = logging.FileHandler(
        os.path.join(
            log_dir, f"minute_download_{datetime.now().strftime('%Y%m%d')}.log"
        ),
        encoding="utf-8",
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    return logger


def backup_progress_file(progress_file: str) -> None:
    """å¤‡ä»½è¿›åº¦æ–‡ä»¶"""
    if not os.path.exists(progress_file):
        return

    backup_file = f"{progress_file}.bak"
    try:
        import shutil

        shutil.copy2(progress_file, backup_file)
        logging.info(f"ğŸ“ å·²å¤‡ä»½è¿›åº¦æ–‡ä»¶åˆ°: {backup_file}")
    except Exception as e:
        logging.warning(f"âš ï¸ å¤‡ä»½è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")


def clear_progress_file(progress_file: str) -> None:
    """æ¸…ç©ºè¿›åº¦æ–‡ä»¶ï¼Œç”¨äºå¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰ETF"""
    try:
        # å…ˆå¤‡ä»½
        backup_progress_file(progress_file)

        # åˆ›å»ºç©ºçš„è¿›åº¦æ–‡ä»¶
        with open(progress_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "downloaded": {},
                    "failed": {},
                    "last_update": "",
                    "last_update_start": "",
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        logging.info("ğŸ§¹ å·²æ¸…ç©ºè¿›åº¦æ–‡ä»¶ï¼Œå°†é‡æ–°ä¸‹è½½æ‰€æœ‰ETFåˆ†é’Ÿæ•°æ®")
    except Exception as e:
        logging.error(f"âŒ æ¸…ç©ºè¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}")
        raise


class ETFMinuteDataDownloader:
    """ETFåˆ†é’Ÿçº§æ•°æ®ä¸‹è½½å·¥å…·ç±»"""

    def __init__(
        self,
        download_dir: str = "download/min",
        progress_file: str = "download/min/etf_minute_progress.json",
        max_retries: int = 3,
        retry_delay: float = 2.0,
        log_level: int = logging.INFO,
        periods: List[str] = ["1", "5", "15"],  # æ”¯æŒçš„åˆ†é’Ÿå‘¨æœŸ
        days_to_download: int = 30,  # é»˜è®¤ä¸‹è½½æœ€è¿‘30å¤©æ•°æ®
    ):
        """
        åˆå§‹åŒ–åˆ†é’Ÿçº§æ•°æ®ä¸‹è½½å™¨

        Args:
            download_dir: æ•°æ®ä¸‹è½½ç›®å½•
            progress_file: è¿›åº¦è®°å½•æ–‡ä»¶è·¯å¾„
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: åŸºç¡€é‡è¯•å»¶è¿Ÿ(ç§’)
            log_level: æ—¥å¿—çº§åˆ«
            periods: è¦ä¸‹è½½çš„åˆ†é’Ÿå‘¨æœŸåˆ—è¡¨
            days_to_download: è¦ä¸‹è½½çš„å†å²å¤©æ•°
        """
        self.download_dir = download_dir
        self.progress_file = progress_file
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.periods = periods
        self.days_to_download = days_to_download

        # åˆ›å»ºä¸‹è½½ç›®å½•
        os.makedirs(self.download_dir, exist_ok=True)
        for period in self.periods:
            os.makedirs(os.path.join(self.download_dir, f"{period}min"), exist_ok=True)

        # é…ç½®æ—¥å¿—
        self.logger = setup_logger(log_level)

        # åŠ è½½è¿›åº¦
        self.progress = self._load_progress()
        self.downloaded_etfs = self.progress.get("downloaded", {})
        self.failed_etfs = self.progress.get("failed", {})

        self.logger.info(f"ETFåˆ†é’Ÿçº§æ•°æ®ä¸‹è½½å·¥å…·åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ä¸‹è½½ç›®å½•: {os.path.abspath(self.download_dir)}")
        self.logger.info(f"åˆ†é’Ÿå‘¨æœŸ: {', '.join(self.periods)}")
        self.logger.info(f"å†å²å¤©æ•°: {self.days_to_download}å¤©")

        # åŒæ­¥å·²æœ‰æ–‡ä»¶
        self._sync_existing_files()

    def _load_progress(self) -> Dict:
        """åŠ è½½ä¸‹è½½è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤è¿›åº¦")

        return {
            "downloaded": {},
            "failed": {},
            "last_update": "",
            "last_update_start": "",
        }

    def _save_progress(self) -> None:
        """ä¿å­˜ä¸‹è½½è¿›åº¦"""
        os.makedirs(os.path.dirname(self.progress_file), exist_ok=True)
        try:
            with open(self.progress_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "downloaded": self.downloaded_etfs,
                        "failed": self.failed_etfs,
                        "last_update": self.progress.get("last_update", ""),
                        "last_update_start": self.progress.get("last_update_start", ""),
                    },
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
            self.logger.debug("è¿›åº¦ä¿å­˜æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {str(e)}")

    def _sync_existing_files(self) -> None:
        """åŒæ­¥å·²æœ‰æ–‡ä»¶åˆ°è¿›åº¦è®°å½•"""
        for period in self.periods:
            period_dir = os.path.join(self.download_dir, f"{period}min")
            if not os.path.exists(period_dir):
                continue

            for filename in os.listdir(period_dir):
                if filename.endswith(".csv"):
                    parts = filename.split("_")
                    if len(parts) >= 2:
                        symbol = parts[0]
                        if symbol.replace(".", "").isdigit():  # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„ETFä»£ç 
                            if symbol not in self.downloaded_etfs:
                                self.downloaded_etfs[symbol] = {}
                            self.downloaded_etfs[symbol][period] = {
                                "last_date": datetime.now().strftime("%Y%m%d"),
                                "status": "completed",
                            }
        self._save_progress()

    def _get_etf_list(self) -> Tuple[pd.DataFrame, Dict[str, str]]:
        """è·å–ETFåˆ—è¡¨"""
        try:
            etf_df = ak.fund_etf_spot_em()
            etf_names = dict(zip(etf_df["ä»£ç "], etf_df["åç§°"]))
            self.logger.info(f"æˆåŠŸè·å– {len(etf_df)} åªETFåŸºç¡€ä¿¡æ¯")
            return etf_df, etf_names
        except Exception as e:
            self.logger.error(f"è·å–ETFåˆ—è¡¨å¤±è´¥: {str(e)}")
            raise

    def _generate_filename(self, symbol: str, name: str, period: str) -> str:
        """ç”Ÿæˆæ ‡å‡†åŒ–çš„æ–‡ä»¶å"""
        clean_name = name
        for char in '*\\/:"?<>|':
            clean_name = clean_name.replace(char, "_")
        return f"{symbol}_{clean_name}_{period}min.csv"

    def _get_date_range(self) -> Tuple[str, str]:
        """è·å–è¦ä¸‹è½½çš„æ—¥æœŸèŒƒå›´"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=self.days_to_download)
        return start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")

    def get_existing_etfs(self) -> List[str]:
        """è·å–å·²ä¸‹è½½çš„ETFä»£ç åˆ—è¡¨"""
        etf_symbols = set()

        for period in self.periods:
            period_dir = os.path.join(self.download_dir, f"{period}min")
            if not os.path.exists(period_dir):
                continue

            for filename in os.listdir(period_dir):
                if filename.endswith(".csv"):
                    symbol = filename.split("_")[0]
                    if symbol.replace(".", "").isdigit():
                        etf_symbols.add(symbol)

        return sorted(list(etf_symbols))

    def _download_single_etf_minute(
        self, symbol: str, name: str, period: str, start_date: str, end_date: str
    ) -> Tuple[bool, Optional[pd.DataFrame]]:
        """
        ä¸‹è½½å•ä¸ªETFçš„åˆ†é’Ÿçº§æ•°æ®

        Args:
            symbol: ETFä»£ç 
            name: ETFåç§°
            period: åˆ†é’Ÿå‘¨æœŸ
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            (æˆåŠŸçŠ¶æ€, æ•°æ®DataFrame)
        """
        # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
        filename = self._generate_filename(symbol, name, period)
        filepath = os.path.join(self.download_dir, f"{period}min", filename)

        self.logger.debug(
            f"å¼€å§‹ä¸‹è½½ {symbol}({name}) {period}åˆ†é’Ÿæ•°æ®: {start_date} è‡³ {end_date}"
        )

        for retry in range(self.max_retries):
            try:
                # è°ƒç”¨akshareæ¥å£è·å–åˆ†é’Ÿæ•°æ®
                df = ak.fund_etf_hist_min_em(
                    symbol=symbol,
                    period=period,
                    start_date=start_date,
                    end_date=end_date,
                    adjust="",
                )

                if df.empty:
                    self.logger.warning(f"ETF {symbol}({name}) {period}åˆ†é’Ÿæ•°æ®ä¸ºç©º")
                    return False, None

                # ä¿å­˜æ•°æ®
                df.to_csv(filepath, index=False, encoding="utf_8_sig")
                self.logger.info(
                    f"æˆåŠŸä¸‹è½½ {symbol}({name}) {period}åˆ†é’Ÿæ•°æ® - {len(df)}æ¡è®°å½•ï¼Œä¿å­˜è‡³ {filename}"
                )
                return True, df

            except Exception as e:
                wait_time = self.retry_delay * (retry + 1)
                self.logger.warning(
                    f"ä¸‹è½½ {symbol}({name}) {period}åˆ†é’Ÿæ•°æ®å¤±è´¥ [{retry+1}/{self.max_retries}]: {str(e)}ï¼Œç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•"
                )
                if retry < self.max_retries - 1:
                    time.sleep(wait_time)

        self.logger.error(
            f"ETF {symbol}({name}) {period}åˆ†é’Ÿæ•°æ®ä¸‹è½½å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°"
        )
        return False, None

    def download_all_etfs(self, symbols: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ä¸‹è½½æ‰€æœ‰ETFçš„åˆ†é’Ÿçº§æ•°æ®

        Args:
            symbols: è¦ä¸‹è½½çš„ETFä»£ç åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä¸‹è½½æ‰€æœ‰

        Returns:
            ç»“æœç»Ÿè®¡å­—å…¸
        """
        start_time = time.time()
        self.progress["last_update_start"] = datetime.now().strftime(
            "%Y-%m-%d %H:%M:%S"
        )
        self._save_progress()

        # è·å–ETFåˆ—è¡¨
        etf_df, etf_names = self._get_etf_list()
        all_symbols = etf_df["ä»£ç "].tolist() if symbols is None else symbols

        # è·å–æ—¥æœŸèŒƒå›´
        start_date, end_date = self._get_date_range()
        self.logger.info(f"ä¸‹è½½æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")

        # ç»Ÿè®¡
        total_etfs = len(all_symbols)
        total_periods = len(self.periods)
        total_tasks = total_etfs * total_periods

        success_count = 0
        fail_count = 0
        failed_details = []

        self.logger.info(
            f"å¼€å§‹ä¸‹è½½ {total_etfs} åªETFçš„åˆ†é’Ÿæ•°æ®ï¼Œå…± {total_tasks} ä¸ªä»»åŠ¡"
        )

        # é€ä¸ªETFä¸‹è½½
        for i, symbol in enumerate(all_symbols, 1):
            name = etf_names.get(symbol, symbol)
            self.logger.info(f"[{i}/{total_etfs}] å¤„ç†ETF: {symbol}({name})")

            # æ¯ä¸ªETFçš„æ¯ä¸ªå‘¨æœŸ
            for period in self.periods:
                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
                if (
                    symbol in self.downloaded_etfs
                    and period in self.downloaded_etfs[symbol]
                ):
                    self.logger.debug(f"  è·³è¿‡ {symbol} {period}åˆ†é’Ÿæ•°æ® (å·²ä¸‹è½½)")
                    continue

                # ä¸‹è½½æ•°æ®
                self.logger.debug(f"  ä¸‹è½½ {symbol} {period}åˆ†é’Ÿæ•°æ®")
                success, _ = self._download_single_etf_minute(
                    symbol, name, period, start_date, end_date
                )

                # æ›´æ–°è¿›åº¦
                if symbol not in self.downloaded_etfs:
                    self.downloaded_etfs[symbol] = {}

                if success:
                    success_count += 1
                    self.downloaded_etfs[symbol][period] = {
                        "last_date": end_date.replace("-", ""),
                        "status": "completed",
                        "download_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    }
                    if symbol in self.failed_etfs and period in self.failed_etfs.get(
                        symbol, {}
                    ):
                        del self.failed_etfs[symbol][period]
                        if not self.failed_etfs[symbol]:
                            del self.failed_etfs[symbol]
                else:
                    fail_count += 1
                    failed_details.append(f"{symbol}_{period}min")
                    if symbol not in self.failed_etfs:
                        self.failed_etfs[symbol] = {}
                    self.failed_etfs[symbol][period] = {
                        "last_attempt": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "reason": "download_failed",
                    }

                # ä¿å­˜è¿›åº¦
                self._save_progress()

                # éµå®ˆè¯·æ±‚é¢‘ç‡é™åˆ¶
                time.sleep(1.5)

            # æ¯å®Œæˆä¸€ä¸ªETFï¼Œæ›´æ–°æ€»è¿›åº¦
            self.progress["last_update"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            self._save_progress()

        # æ€»ç»“
        elapsed = time.time() - start_time
        self.logger.info(
            f"ä¸‹è½½å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}, è€—æ—¶: {elapsed:.1f}ç§’"
        )

        return {
            "success_count": success_count,
            "fail_count": fail_count,
            "failed_details": failed_details,
            "total_time": elapsed,
            "total_etfs": total_etfs,
            "total_periods": total_periods,
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ETFåˆ†é’Ÿçº§æ•°æ®ä¸‹è½½å·¥å…·")
    parser.add_argument(
        "--full", action="store_true", help="å¼ºåˆ¶å…¨é‡ä¸‹è½½ï¼ˆå¿½ç•¥å·²æœ‰æ•°æ®ï¼‰"
    )
    parser.add_argument(
        "--full-history", action="store_true", help="ä¸‹è½½å®Œæ•´å†å²æ•°æ®ï¼ˆæ¸…ç©ºè¿›åº¦è®°å½•ï¼‰"
    )
    parser.add_argument(
        "--update", action="store_true", help="ä»…æ›´æ–°å·²æœ‰ETFæ•°æ®ï¼ˆå¢é‡æ›´æ–°ï¼‰"
    )
    parser.add_argument("--symbol", type=str, help="æŒ‡å®šå•ä¸ªETFä»£ç ï¼Œä¾‹å¦‚: 513500")
    parser.add_argument(
        "--symbols-file", type=str, help="ä»æ–‡ä»¶è¯»å–ETFä»£ç åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªä»£ç "
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="æ—¥å¿—çº§åˆ«",
    )
    parser.add_argument(
        "--download-dir", type=str, default="download/min", help="ä¸‹è½½ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--progress-file",
        type=str,
        default="download/min/etf_minute_progress.json",
        help="è¿›åº¦æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument("--max-retries", type=int, default=3, help="æœ€å¤§é‡è¯•æ¬¡æ•°")
    parser.add_argument(
        "--retry-delay", type=float, default=2.0, help="åŸºç¡€é‡è¯•å»¶è¿Ÿ(ç§’)"
    )
    parser.add_argument("--days", type=int, default=60, help="è¦ä¸‹è½½çš„å†å²å¤©æ•°")
    parser.add_argument(
        "--periods", type=str, default="1,5,15", help="è¦ä¸‹è½½çš„åˆ†é’Ÿå‘¨æœŸï¼Œç”¨é€—å·åˆ†éš”"
    )

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    log_level = getattr(logging, args.log_level.upper())
    logger = setup_logger(log_level)

    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    logger.info("=" * 60)
    logger.info("ğŸš€ ETFåˆ†é’Ÿçº§æ•°æ®ä¸‹è½½å·¥å…·")
    logger.info(f"ç‰ˆæœ¬: 1.0")
    logger.info(f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ä¸‹è½½ç›®å½•: {os.path.abspath(args.download_dir)}")
    logger.info(f"è¿›åº¦æ–‡ä»¶: {os.path.abspath(args.progress_file)}")
    logger.info(f"å†å²å¤©æ•°: {args.days}å¤©")
    logger.info(f"åˆ†é’Ÿå‘¨æœŸ: {args.periods}")
    logger.info("=" * 60)

    # å¤„ç†å‚æ•°
    periods = [p.strip() for p in args.periods.split(",")]
    symbols = None

    if args.symbol:
        symbols = [args.symbol.strip()]
        logger.info(f"ğŸ¯ æŒ‡å®šETF: {args.symbol}")
    elif args.symbols_file and os.path.exists(args.symbols_file):
        with open(args.symbols_file, "r", encoding="utf-8") as f:
            symbols = [
                line.strip() for line in f if line.strip() and not line.startswith("#")
            ]
        logger.info(f"ğŸ“‹ ä»æ–‡ä»¶åŠ è½½ {len(symbols)} ä¸ªETFä»£ç ")

    # å¦‚æœéœ€è¦ä¸‹è½½å®Œæ•´å†å²ï¼Œæ¸…ç©ºè¿›åº¦æ–‡ä»¶
    if args.full and args.full_history:
        if os.path.exists(args.progress_file):
            clear_progress_file(args.progress_file)
        else:
            logger.info("â„¹ï¸ è¿›åº¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")

    try:
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = ETFMinuteDataDownloader(
            download_dir=args.download_dir,
            progress_file=args.progress_file,
            max_retries=args.max_retries,
            retry_delay=args.retry_delay,
            log_level=log_level,
            periods=periods,
            days_to_download=args.days,
        )

        # ç¡®å®šè¿è¡Œæ¨¡å¼
        mode = "full" if args.full else "update"
        if not args.full and not args.update:
            mode = "auto"
            existing_etfs = downloader.get_existing_etfs()
            if not existing_etfs:
                mode = "full"
                logger.info("æœªå‘ç°å·²æœ‰æ•°æ®ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å…¨é‡ä¸‹è½½æ¨¡å¼")
            else:
                mode = "update"
                logger.info(
                    f"å‘ç° {len(existing_etfs)} ä¸ªå·²æœ‰ETFæ•°æ®ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¢é‡æ›´æ–°æ¨¡å¼"
                )

        # æ‰§è¡Œä¸‹è½½
        logger.info("\n" + "=" * 60)
        logger.info(f"âš¡ å¼€å§‹æ‰§è¡Œ {mode} æ“ä½œ")
        logger.info("=" * 60)

        start_time = time.time()
        result = downloader.download_all_etfs(symbols)
        elapsed_time = time.time() - start_time

        # æ˜¾ç¤ºç»“æœ
        logger.info("\n" + "=" * 60)
        logger.info("âœ… ä»»åŠ¡å®Œæˆ!")
        logger.info(f"æˆåŠŸ: {result['success_count']}, å¤±è´¥: {result['fail_count']}")
        logger.info(f"è€—æ—¶: {elapsed_time:.1f} ç§’")
        logger.info(f"è¿›åº¦å·²ä¿å­˜åˆ°: {args.progress_file}")
        logger.info("=" * 60)

        # æ˜¾ç¤ºå¤±è´¥åˆ—è¡¨
        if result.get("fail_count", 0) > 0:
            failed_details = result.get("failed_details", [])
            logger.error(f"\nâŒ {len(failed_details)} ä¸ªETFåˆ†é’Ÿæ•°æ®ä¸‹è½½å¤±è´¥:")
            for i, detail in enumerate(failed_details[:10], 1):
                logger.error(f" {i}. {detail}")
            if len(failed_details) > 10:
                logger.error(f" ... è¿˜æœ‰ {len(failed_details)-10} ä¸ªå¤±è´¥çš„ä»»åŠ¡")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        existing_etfs = downloader.get_existing_etfs()
        logger.info(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        logger.info(f" æ€»ETFæ•°é‡: {len(existing_etfs)}")
        logger.info(f" æœ¬æ¬¡æˆåŠŸ: {result['success_count']}")

        return 0

    except KeyboardInterrupt:
        logger.warning("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œè¿›åº¦å·²è‡ªåŠ¨ä¿å­˜")
        logger.warning("ä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­")
        return 1
    except Exception as e:
        logger.exception(f"\nâŒ ä¸¥é‡é”™è¯¯: {str(e)}")
        logger.error("è¿›åº¦å·²ä¿å­˜ï¼Œä¿®å¤é—®é¢˜åå¯ç»§ç»­è¿è¡Œ")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
